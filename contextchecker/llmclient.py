import asyncio
import json
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Any, Optional
from openai import (
    AsyncOpenAI,
    APIError, APIStatusError, APIConnectionError, APITimeoutError,
    AuthenticationError, PermissionDeniedError, BadRequestError,
    NotFoundError, ConflictError, UnprocessableEntityError,
    RateLimitError, InternalServerError,
)
from tqdm.asyncio import tqdm_asyncio
from sys import exit
from contextchecker.stats import GLOBAL_STATS


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  RETRY MATRIX
#  Each strategy is a complete request configuration.
#  On capability errors, we advance to the next strategy.
#  First real request discovers the working strategy (serialized).
#  All subsequent requests use the locked strategy concurrently.
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@dataclass
class RetryStrategy:
    """One level in the retry matrix. Readable and explicit."""
    name: str
    reasoning_effort: Optional[str] = None   # "low", "medium", "high" ‚Äî OpenAI standard. None = don't send.
    use_schema: bool = True                  # Strict JSON Schema (structured output, constrained decoding)
    use_json_object: bool = False            # Loose JSON (valid JSON, shape not enforced)
    temperature: float = 0.0


# Best case at top, vanilla at bottom.
# On capability errors (BadRequest, UnsupportedParams), we walk down.
RETRY_MATRIX = [
    RetryStrategy("Reasoning + Schema",  reasoning_effort="low",  use_schema=True),
    RetryStrategy("Schema Only",                                  use_schema=True),
    RetryStrategy("Reasoning + JSON",    reasoning_effort="low",  use_json_object=True),
    RetryStrategy("JSON Only",                                    use_json_object=True),
    RetryStrategy("Vanilla")
]


class ErrorAction(Enum):
    """What to do when an API error occurs."""
    FATAL = "fatal"   # Exit program ‚Äî unrecoverable
    SKIP  = "skip"    # Return "", continue batch ‚Äî per-item failure
    RETRY = "retry"   # Backoff and retry ‚Äî transient


class LLMClient:
    def __init__(self, api_key: str, model: str, base_url: Optional[str] = None, concurrency: int = 10):
        self.base_url = base_url
        self.api_key = api_key
        self.model = model
        self.concurrency = concurrency
        
        # OpenAI SDK client ‚Äî only created if base_url is set (direct endpoint mode)
        if self.base_url:
            self.client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)
        else:
            self.client = None  # LiteLLM mode ‚Äî no direct client needed
        
        self._connection_verified = False
        self.sem = asyncio.Semaphore(self.concurrency)

        # Retry matrix state (OpenAI SDK path only)
        self._strategy_index = 0
        self._strategy_discovered = False   # True after first successful request
        self._discovery_lock = asyncio.Lock()  # Serializes strategy discovery
        self._cache_hit_logged = False  # Only log cache hint once

        sdk_mode = "OpenAI SDK" if self.base_url else "LiteLLM"
        print(f"üîß LLMClient initialized: {self.model} via {sdk_mode}")


    @property
    def strategy(self) -> RetryStrategy:
        """Current retry strategy."""
        return RETRY_MATRIX[self._strategy_index]


    def _next_strategy(self) -> bool:
        """Advance to next strategy. Returns True if advanced, False if at bottom."""
        if self._strategy_index < len(RETRY_MATRIX) - 1:
            self._strategy_index += 1
            print(f"   ‚¨áÔ∏è  Next strategy: '{self.strategy.name}'")
            return True
        return False


    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    #  CENTRAL ERROR HANDLER
    #  Order matters! Subclasses MUST be checked before parents.
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _handle_api_error(self, e: Exception, attempt: int = 0, max_retries: int = 0) -> ErrorAction:

        # ‚îÄ‚îÄ FATAL: Auth / Permissions / Not Found ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        if isinstance(e, AuthenticationError):
            print(f"\n‚õî AUTH ERROR ({self.model})")
            print(f"   API Key rejected or expired.")
            print(f"   Key: {self.api_key[:6]}...")
            print(f"   Error: {e}")
            return ErrorAction.FATAL

        if isinstance(e, PermissionDeniedError):
            print(f"\n‚õî PERMISSION DENIED ({self.model})")
            print(f"   Your API key is valid but lacks access to this resource.")
            print(f"   Check your plan/tier or model permissions.")
            print(f"   Error: {e}")
            return ErrorAction.FATAL

        if isinstance(e, NotFoundError):
            print(f"\n‚õî NOT FOUND: Model '{self.model}' does not exist on {self.base_url}")
            print(f"   Error: {e}")
            return ErrorAction.FATAL

        if e.__class__.__name__ == 'BudgetExceededError':
            print(f"\n‚õî BUDGET EXCEEDED ‚Äî LiteLLM proxy budget limit reached.")
            print(f"   Error: {e}")
            return ErrorAction.FATAL

        # ‚îÄ‚îÄ SKIP: Per-item failures ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        if e.__class__.__name__ == 'ContextWindowExceededError':
            print(f"‚ö†Ô∏è  CONTEXT WINDOW EXCEEDED ({self.model}): Input too long. Skipping.")
            print(f"   Details: {str(e)[:300]}")
            return ErrorAction.SKIP

        if e.__class__.__name__ == 'ContentPolicyViolationError':
            print(f"‚ö†Ô∏è  CONTENT POLICY VIOLATION ({self.model}): Safety filter triggered. Skipping.")
            print(f"   Details: {str(e)[:300]}")
            return ErrorAction.SKIP

        if e.__class__.__name__ == 'UnsupportedParamsError':
            print(f"‚ö†Ô∏è  UNSUPPORTED PARAMS ({self.model}): {str(e)[:300]}")
            return ErrorAction.SKIP

        if e.__class__.__name__ == 'JSONSchemaValidationError':
            print(f"‚ö†Ô∏è  SCHEMA VALIDATION FAILED ({self.model}): {str(e)[:300]}")
            return ErrorAction.SKIP

        if isinstance(e, UnprocessableEntityError):
            print(f"‚ö†Ô∏è  UNPROCESSABLE ENTITY ({self.model}): {str(e)[:300]}")
            return ErrorAction.SKIP

        # ‚îÄ‚îÄ CONFIG ERROR: BadRequest base (after subclass checks!) ‚îÄ

        if isinstance(e, BadRequestError):   
            error_text = ""
            if hasattr(e, 'body') and isinstance(e.body, dict):
                error_text = str(e.body).lower()
            else:
                error_text = str(e).lower()        
            if "invalid model" in error_text or "model name" in error_text:
                print(f"\n‚õî CRITICAL: Model Error for '{self.model}' - Not found!")
                
                # 3. String-Catching f√ºr den Prefix (wie von dir gefordert)
                if "/" in self.model:
                    prefix, actual_model = self.model.split("/", 1)
                    print(f"üí° HINT: You are using the prefix '{prefix}/'.")
                    print(f"   A possible error cause is that when using a custom base_url (Proxy/Local),")
                    print(f"   you MUST NOT use a provider prefix, since it is not using LiteLLM. The provider information is only for the LiteLLM SDK.")
                    print(f"   -> If that is the case: Change model to '{actual_model}' instead of '{self.model}'\n")
                else:
                    print(f"üí° HINT: The model name was rejected by your base_url. Call `/v1/models` to check available models.\n")
                
                return ErrorAction.FATAL
        
            # Fallback f√ºr alle anderen 400er Fehler (Context Window zu gro√ü, falsches Schema etc.)
            print(f"‚ö†Ô∏è BAD REQUEST ({self.model}): {str(e)[:300]}")
            return ErrorAction.SKIP

        # ‚îÄ‚îÄ RETRY: Transient errors ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        retry_label = f"Attempt {attempt + 1}/{max_retries + 1}"

        if isinstance(e, RateLimitError):
            print(f"üîÑ RATE LIMITED ({self.model}) ‚Äî {retry_label}")
            return ErrorAction.RETRY

        if isinstance(e, APITimeoutError):
            print(f"üîÑ TIMEOUT ({self.model}) ‚Äî {retry_label}")
            return ErrorAction.RETRY

        if isinstance(e, APIConnectionError):
            print(f"üîÑ CONNECTION ERROR ({self.model}) ‚Äî {retry_label}")
            return ErrorAction.RETRY

        if isinstance(e, InternalServerError) or e.__class__.__name__ == 'ServiceUnavailableError':
            print(f"üîÑ SERVER ERROR ({self.model}) ‚Äî {retry_label}")
            return ErrorAction.RETRY

        if isinstance(e, ConflictError):
            print(f"üîÑ CONFLICT ({self.model}) ‚Äî {retry_label}")
            return ErrorAction.RETRY

        if isinstance(e, APIError):
            # Generic APIError fallback ‚Äî treat as retryable

            # 1. Spezifischer Check auf 402 (Insufficient Credits / Payment Required)
            status_code = getattr(e, "status_code", None)
            
            if status_code == 402:
                print(f"\n‚õî CRITICAL ERROR (402): Out of Credits or Context too large for {self.model}.")
                print(f"    Error: {e}")
                return ErrorAction.FATAL

            # 2. Generic APIError fallback ‚Äî treat as retryable (z.B. 500, 502)
            print(f"üîÑ API ERROR ({self.model}) ‚Äî {retry_label}: {str(e)[:300]}")
            return ErrorAction.RETRY

        # ‚îÄ‚îÄ UNKNOWN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        print(f"üí• UNEXPECTED ERROR ({self.model}): {type(e).__name__}: {str(e)[:300]}")
        return ErrorAction.SKIP


    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    #  GENERATE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def generate(self, messages: List[Dict], schema: Any = None, max_retries=2, **kwargs) -> str:
        """
        Runs one LLM request.
        - base_url set     ‚Üí OpenAI SDK (direct endpoint, no provider prefix needed)
        - base_url not set ‚Üí LiteLLM (provider routing via model prefix, e.g. 'openrouter/...')
        - First request discovers the best strategy (serialized via lock).
        - All subsequent requests use the locked strategy concurrently.
        """
        if not self._connection_verified:
            await self.check_connection()

        # ‚îÄ‚îÄ Discovery: serialize the first request to walk the matrix alone ‚îÄ‚îÄ
        # All other requests wait at the lock until discovery is done.
        discovering = False
        if self.base_url and not self._strategy_discovered:
            await self._discovery_lock.acquire()
            if self._strategy_discovered:
                # Someone else discovered while we waited ‚Äî release and continue
                self._discovery_lock.release()
            else:
                discovering = True
                print(f"üî¨ Discovering best strategy for {self.model}...")

        try:
            async with self.sem:
                last_error = None
                attempt = 0

                while attempt <= max_retries:
                    try:
                        if self.base_url:
                            # ‚îÄ‚îÄ OpenAI SDK Path ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            # kwargs go in first, strategy overwrites on top
                            strategy = self.strategy
                            call_kwargs = {
                                "model": self.model,
                                "messages": messages,
                                **kwargs,
                                "temperature": strategy.temperature,
                            }

                            # Strategy controls reasoning ‚Äî always overwrites
                            if strategy.reasoning_effort:
                                call_kwargs["reasoning_effort"] = strategy.reasoning_effort
                            else:
                                call_kwargs.pop("reasoning_effort", None)

                            # Strategy controls output format ‚Äî always overwrites
                            if schema:
                                if strategy.use_schema:
                                    call_kwargs["response_format"] = schema
                                elif strategy.use_json_object:
                                    call_kwargs["response_format"] = {"type": "json_object"}
                                else:
                                    # Vanilla mode ‚Äî no response_format, inject JSON instructions into prompt. Temporary solution
                                    call_kwargs.pop("response_format", None)
                                    schema_json = json.dumps(schema.model_json_schema(), indent=2)
                                    patched_messages = list(messages)
                                    patched_messages[-1] = {
                                        **patched_messages[-1],
                                        "content": patched_messages[-1]["content"]
                                            + f"\n\nRespond ONLY with valid JSON matching this schema:\n{schema_json}"
                                    }
                                    call_kwargs["messages"] = patched_messages

                            response = await self.client.chat.completions.parse(**call_kwargs)

                        else:
                            # ‚îÄ‚îÄ LiteLLM Path (no matrix, passthrough) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                            import litellm
                            litellm.suppress_debug_info = True
                            from litellm import acompletion

                            call_kwargs = {
                                "model": self.model,
                                "messages": messages,
                                "api_key": self.api_key,
                                "drop_params": False,
                                **kwargs
                            }
                            if schema:
                                call_kwargs["response_format"] = schema

                            response = await acompletion(**call_kwargs)

                        # ‚îÄ‚îÄ Success ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                        if hasattr(response, 'usage') and response.usage:
                            GLOBAL_STATS.update(response.usage.model_dump())

                        # Lock strategy on first success
                        if discovering and not self._strategy_discovered:
                            self._strategy_discovered = True
                            print(f"   üîí Strategy locked: '{self.strategy.name}'")

                        # Cache hint (only log once to avoid spam)
                        if not self._cache_hit_logged:
                            cache_hit = getattr(response, '_hidden_params', {}).get('cache_hit', False)
                            if cache_hit:
                                print(f"   üíæ Cache hit detected ‚Äî provider is caching responses.")
                                self._cache_hit_logged = True

                        return response.choices[0].message.content

                    except Exception as e:
                        action = self._handle_api_error(e, attempt, max_retries)

                        # During discovery: advance strategy on capability errors
                        # This does NOT count as a retry attempt except if it is a fatal error

                        if action == ErrorAction.FATAL:
                            exit(f"FATAL: {type(e).__name__} ‚Äî Cannot continue.")

                        is_capability_error = (
                            isinstance(e, BadRequestError) or e.__class__.__name__ == 'UnsupportedParamsError'
                        ) and not (
                            e.__class__.__name__ in ('ContextWindowExceededError', 'ContentPolicyViolationError')
                        )

                        if is_capability_error and discovering and self._next_strategy():
                            continue  # same attempt counter, just different strategy


                        elif action == ErrorAction.SKIP:
                            GLOBAL_STATS.log_error()
                            return ""

                        elif action == ErrorAction.RETRY:
                            if attempt < max_retries:
                                wait_time = 0.5 * (attempt + 1)
                                print(f"   ‚è≥ Waiting {wait_time}s before retry...")
                                await asyncio.sleep(wait_time)
                                attempt += 1
                                continue
                            else:
                                last_error = e
                                break

                # All retries exhausted
                print(f"üî¥ FAILED after {attempt + 1} attempts. Last error: {str(last_error)[:100]}")
                GLOBAL_STATS.log_error()
                return ""

        finally:
            # Release the discovery lock if we hold it
            if discovering:
                self._strategy_discovered = True  # lock at whatever level, even on failure
                if self._discovery_lock.locked():
                    self._discovery_lock.release()


    async def generate_batch(self, tasks_data: List[Dict], description="Processing") -> List[str]:
        """
        Batch helper. Expects a list of dicts with args for self.generate(), e.g.:
        [{'messages': [...], 'schema': MyModel}, ...]
        """
        if not self._connection_verified:
            await self.check_connection()

        tasks = [self.generate(**task_args) for task_args in tasks_data]
        return await tqdm_asyncio.gather(*tasks, desc=description)


    async def check_connection(self):
        """Pre-flight check: verifies API reachability and authentication."""
        if not self.base_url:
            # LiteLLM mode ‚Äî no direct endpoint to check, skip pre-flight
            print(f"üì° LiteLLM mode ({self.model}) ‚Äî skipping pre-flight connection check.")
            self._connection_verified = True
            return

        print(f"üì° Testing connection to {self.base_url}/models...")
        try:
            await self.client.models.list()
            print("   ‚úÖ Connection confirmed. Server reachable")
            self._connection_verified = True

        except AuthenticationError as e:
            print(f"\n‚ùå FATAL: Authentication Failed.")
            print(f"   Key: {self.api_key[:6]}...")
            print(f"   Error: {e}")
            exit("FATAL: Auth Error ‚Äî check your API key.")

        except PermissionDeniedError as e:
            print(f"\n‚ùå FATAL: Permission Denied.")
            print(f"   Your key is valid but cannot access this endpoint.")
            print(f"   Error: {e}")
            exit("FATAL: Permission Denied ‚Äî check your API plan/tier.")

        except NotFoundError as e:
            # /v1/models may not exist on custom endpoints (vllm, sglang, etc.)
            print(f"   ‚ö° /models endpoint not available ‚Äî skipping pre-flight check.")
            print(f"   (This is normal for custom providers like vllm, sglang, etc.)")
            self._connection_verified = True

        except APIConnectionError as e:
            print(f"\n‚ùå FATAL: Cannot connect to API endpoint.")
            print(f"   URL: {self.base_url}")
            print(f"   Error: {e}")
            print(f"   Check: Is the URL correct? Is the server running? Firewall/proxy issues?")
            print(f"   Skip modell check with ---------------------------------------------------------------arg")
            exit(f"FATAL: Cannot connect to {self.base_url}")

        except APITimeoutError as e:
            print(f"\n‚ùå FATAL: Connection timed out during pre-flight check.")
            print(f"   URL: {self.base_url}")
            print(f"   Error: {e}")
            exit(f"FATAL: Timeout connecting to {self.base_url}")

        except Exception as e:
            print(f"\n‚ùå FATAL: Unexpected error during connection check.")
            print(f"   Type: {type(e).__name__}")
            print(f"   Error: {str(e)}")
            exit(f"FATAL: {type(e).__name__} in check_connection: {e}")